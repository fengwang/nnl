#include "../../include/direct_space/graph.hpp"
#include "../../include/direct_space/node.hpp"
#include "../../include/direct_space/computation_table.hpp"
#include "../../include/direct_space/allocator.hpp"
#include "../../include/direct_space/engine.hpp"
#include "../../include/direct_space/stream.hpp"
#include "../../include/utility/wheels/cached_allocator.hpp"
#include "../../include/direct_space/session.hpp"
#include "../../include/direct_space/device.hpp"
#include "../../include/direct_space/tensor.hpp"
#include "../../include/utility/wheels/view.hpp"
#include "../../include/direct_space/layer.hpp"
#include "../../include/direct_space/model.hpp"

TEST_CASE( "Multi-head-attention-model-39", "[multi-head-attention-model-39]" )
{
    spdlog::info( "\nTest case of 39 started.\n" );

    std::this_thread::sleep_for( std::chrono::seconds( 1 ) );

    using namespace nnl;
    auto& sess = get_default_session<default_engine_type>();
    sess.clean();

    std::int64_t const n_seq = 2;
    std::int64_t const n_head = 2;
    std::int64_t const n_embd = 6;

    auto input = make_tensor<default_engine_type>( {n_seq, n_embd}, "float32", "tensor_input" );
    auto w_att = make_tensor<default_engine_type>( {n_embd, n_embd*3}, "float32", "tensor_w_att" );
    auto b_att = make_tensor<default_engine_type>( {n_embd*3,}, "float32", "tensor_b_att" );
    auto w_proj = make_tensor<default_engine_type>( {n_embd, n_embd}, "float32", "tensor_w_proj" );
    auto b_proj = make_tensor<default_engine_type>( {n_embd,}, "float32", "tensor_b_proj" );
    auto gt = make_tensor<default_engine_type>( {n_seq, n_embd}, "float32", "tensor_gt" );



    // shape is (2, 6)
    [[maybe_unused]] float d_input[]=
    {
    	-0.5534178416929223,	0.046326682801352215,	0.10140291316236216,	-0.9087960997343874,	-0.27854232930445466,	-0.5538381166174224,	0.377452323642713,	-0.6725371499982016,	-0.8593502663889994,	0.882021720491672,	0.12736276046965056,	-0.8440153212200259,
    };

    // shape is (6, 18)
    [[maybe_unused]] float d_attn_w[]=
    {
    	0.4452810222496797,	-0.6830956529536671,	-0.49943738651195324,	-0.413025487795605,	0.39322142841718977,	-0.07147182391969653,	-0.569875712571321,	-0.10634747584068482,	-0.7582424969220862,	0.83537401548945,	0.13289265743839107,	-0.05596136851614553,	-0.37086715070557696,	-0.9135685405267477,	0.5048898339199461,	0.08653808774263605,	0.5343808514294013,	0.5808975324315737,	-0.8555572412507397,	0.31943166881569507,	-0.8934013233115059,	0.3700489054454015,	-0.1132980759734703,	-0.06400770524007782,	0.32211866952991897,	0.3799332638532238,	-0.4733410311088724,	-0.9770421324796503,	0.5425888932495304,	0.6322846525776988,	-0.7207167925034084,	0.15601349114968222,	-0.3186514559116014,	-0.535569025061867,	-0.6027124928297907,	-0.4570184534226387,	0.953705528021386,	-0.5691420434372769,	-0.28718516587580756,	-0.9632001067387674,	0.2335805391900283,	-0.5391892244378311,	0.4847034721316992,	0.168500416789773,	-0.15979289082048598,	0.41850566438972425,	0.9440910518067389,	0.6200782353145564,	-0.13868964067662737,	0.9154571996229015,	-0.9044837977902083,	0.46556360178337663,	0.35133289856278926,	0.05401033558831192,	0.1339749037447604,	0.445890498494282,	0.3652067003305257,	0.5906253601985791,	-0.7743089555964124,	-0.6016747356572583,	0.44263845794550716,	-0.21226535503433608,	-0.3216291937643605,	-0.6931086278567578,
    	-0.798851970777563,	-0.1855057862898446,	0.16583092893300755,	0.4814410419233621,	0.7407216835513775,	0.9330523208053558,	-0.0888702892955664,	-0.1991540018566338,	-0.4021207380731766,	0.6567201334862367,	0.8196823399815116,	0.5322733505025901,	-0.86337043137215,	0.2120530399516265,	0.8784322648567571,	-0.795714684509865,	0.9550790328799044,	-0.2866225512510203,	0.30837433667812064,	-0.7544285874396095,	0.9227043868271638,	0.3023628543432315,	0.8733556197886474,	0.817654610548818,	-0.12598437753173974,	0.8665219187748021,	0.41243449688471534,	-0.4657267385142949,	-0.6404671971680762,	0.31110670971769516,	0.644055632533985,	0.8158177341712622,	0.2523514431062559,	-0.6744549249012155,	-0.43670761373143496,	0.17641814589723892,	-0.7765011812575202,	-0.46095543509259973,	-0.9960311979084255,	0.793126748094007,	0.8606549164375583,	-0.31953209432521024,	-0.15483789828661543,	0.3403927262924109,
    };

    // shape is (18,)
    [[maybe_unused]] float d_attn_b[]=
    {
    	0.22247718304056163,	0.32184601225666576,	0.014125608009941226,	0.2699465098949414,	-0.7835174969795935,	0.9024014311513453,	-0.9331369713482778,	-0.8059820672082039,	0.6938935363417189,	0.22473750791601455,	0.40030290793377854,	0.10052025567236811,	0.6228028307496731,	0.2887641511978569,	-0.6050456288620749,	0.5466453239697793,	-0.3107915580566094,	0.7008204675109508,
    };

    // shape is (6, 6)
    [[maybe_unused]] float d_proj_w[]=
    {
    	-0.3174037097123865,	0.9435332700341295,	-0.5531408522308381,	0.2767998655895729,	-0.7252857172015899,	-0.7412018408044516,	0.30255146121218757,	0.4348328252113165,	-0.8010240572240188,	0.15505132828464085,	-0.1709343440354727,	-0.7296280388513319,	0.03522771750395237,	-0.8705504627968308,	0.7964160613190279,	-0.8111254245821189,	0.7257071617767945,	0.47694416094714853,	-0.770976947824699,	0.0853750818407768,	0.9403013445905344,	-0.5058455174138803,	0.1468515264773531,	0.1360702461191019,	-0.3170149438694583,	-0.7645466800583802,	0.08900728280904135,	0.13572990918998928,	0.8290795294590052,	0.252801252383835,	-0.008752175622826597,	-0.6291109360156826,	-0.22244175217126294,	0.14540858053651418,	0.45458378043581615,	0.9754669326636882,
    };

    // shape is (6,)
    [[maybe_unused]] float d_proj_b[]=
    {
    	-0.6164227961281241,	0.3459680040385973,	-0.16933237088724318,	0.2730163508786929,	-0.9880592690960197,	0.4665912047641243,
    };

    // shape is (2, 6)
    [[maybe_unused]] float d_gt[]=
    {
    	-0.5979781813231169,	3.4639978805857012,	-2.9419717300735444,	2.6061178703655545,	-3.709927812266841,	-1.346971363731294,	-2.3876488997725387,	2.402202520101394,	-0.535115981847073,	1.1602416935393909,	-2.4102579937679645,	0.002731274502601133,
    };


    input.import_from( d_input );
    input.save_txt( "./input.txt" );
    w_att.import_from( d_attn_w );
    w_att.save_txt( "./w_att.txt" );
    b_att.import_from( d_attn_b );
    b_att.save_txt( "./b_att.txt" );
    w_proj.import_from( d_proj_w );
    w_proj.save_txt( "./w_proj.txt" );
    b_proj.import_from( d_proj_b );
    b_proj.save_txt( "./b_proj.txt" );
    gt.import_from( d_gt );
    gt.save_txt( "./gt.txt" );

    auto input_layer = Input( "InputLayer" );
    auto output_layer = MultiHeadAttention( w_att, b_att, w_proj, b_proj, n_head, "Attention_Layer_model_39" )( input_layer );
    auto m = model( input_layer, output_layer );
    auto outputs = m.predict( input );
    auto output = outputs[0];

    output.save_txt( "./output.txt" );

    auto mat = view_2d{ reinterpret_cast<float*>( gt.data() ), static_cast<std::uint64_t>(n_seq), static_cast<std::uint64_t>(n_embd) };
    auto nat = view_2d{ reinterpret_cast<float*>( output.data() ), static_cast<std::uint64_t>(n_seq), static_cast<std::uint64_t>(n_embd) };
    for ( auto r : range( n_seq ) )
        for ( auto c : range( n_embd ) )
        {
            if ( std::abs(mat[r][c]-nat[r][c]) > 1.0e-2 )
            {
                spdlog::error( "gt[{}][{}]={}, pred[{}][{}]={}", r, c, mat[r][c], r, c, nat[r][c] );
            }
            REQUIRE( std::abs(mat[r][c]-nat[r][c]) < 1.0e-2 );
        }
}

