#include "../../include/direct_space/graph.hpp"
#include "../../include/direct_space/node.hpp"
#include "../../include/direct_space/computation_table.hpp"
#include "../../include/direct_space/allocator.hpp"
#include "../../include/direct_space/engine.hpp"
#include "../../include/direct_space/stream.hpp"
#include "../../include/utility/wheels/cached_allocator.hpp"
#include "../../include/direct_space/session.hpp"
#include "../../include/direct_space/device.hpp"
#include "../../include/direct_space/tensor.hpp"
#include "../../include/utility/wheels/view.hpp"
#include "../../include/direct_space/layer.hpp"
#include "../../include/direct_space/model.hpp"

extern "C" void cuda_device_synchronize();

TEST_CASE( "Multi-head-attention-model-43", "[multi-head-attention-model-43]" )
{
    spdlog::info( "\nTest case of 43 started.\n" );

    using namespace nnl;
    auto& sess = get_default_session<default_engine_type>();
    sess.clean();

    std::int64_t const n_seq = 2;
    std::int64_t const n_embd = 6;
    std::int64_t const n_head = 2;

    auto input = make_tensor<default_engine_type>( {n_seq, n_embd}, "float32", "input_tensor" );

    auto mlp_c_fc_w = make_tensor<default_engine_type>( {n_embd, n_embd}, "float32" );
    auto mlp_c_fc_b = make_tensor<default_engine_type>( {n_embd,}, "float32" );
    auto mlp_c_proj_w = make_tensor<default_engine_type>( {n_embd, n_embd}, "float32" );
    auto mlp_c_proj_b = make_tensor<default_engine_type>( {n_embd,}, "float32" );

    auto mha_att_w = make_tensor<default_engine_type>( {n_embd, n_embd*3}, "float32" );
    auto mha_att_b = make_tensor<default_engine_type>( {n_embd*3,}, "float32" );
    auto mha_proj_w = make_tensor<default_engine_type>( {n_embd, n_embd}, "float32" );
    auto mha_proj_b = make_tensor<default_engine_type>( {n_embd,}, "float32" );

    auto ln_1_alpha = make_tensor<default_engine_type>( {n_embd,}, "float32" );
    auto ln_1_beta = make_tensor<default_engine_type>( {n_embd,}, "float32" );
    auto ln_2_alpha = make_tensor<default_engine_type>( {n_embd,}, "float32" );
    auto ln_2_beta = make_tensor<default_engine_type>( {n_embd,}, "float32" );

    auto gt = make_tensor<default_engine_type>( {n_seq, n_embd,}, "float32" );


    // shape is (2, 6)
    [[maybe_unused]] float d_input[]=
    {
    	-0.5534178416929223,	0.046326682801352215,	0.10140291316236216,	-0.9087960997343874,	-0.27854232930445466,	-0.5538381166174224,	0.377452323642713,	-0.6725371499982016,	-0.8593502663889994,	0.882021720491672,	0.12736276046965056,	-0.8440153212200259,
    };

    // shape is (6,)
    [[maybe_unused]] float d_ln_1_alpha[]=
    {
    	0.4452810222496797,	-0.6830956529536671,	-0.49943738651195324,	-0.413025487795605,	0.39322142841718977,	-0.07147182391969653,
    };

    // shape is (6,)
    [[maybe_unused]] float d_ln_1_beta[]=
    {
    	-0.569875712571321,	-0.10634747584068482,	-0.7582424969220862,	0.83537401548945,	0.13289265743839107,	-0.05596136851614553,
    };

    // shape is (6,)
    [[maybe_unused]] float d_ln_2_alpha[]=
    {
    	-0.37086715070557696,	-0.9135685405267477,	0.5048898339199461,	0.08653808774263605,	0.5343808514294013,	0.5808975324315737,
    };

    // shape is (6,)
    [[maybe_unused]] float d_ln_2_beta[]=
    {
    	-0.8555572412507397,	0.31943166881569507,	-0.8934013233115059,	0.3700489054454015,	-0.1132980759734703,	-0.06400770524007782,
    };

    // shape is (6, 18)
    [[maybe_unused]] float d_mha_att_w[]=
    {
    	0.32211866952991897,	0.3799332638532238,	-0.4733410311088724,	-0.9770421324796503,	0.5425888932495304,	0.6322846525776988,	-0.7207167925034084,	0.15601349114968222,	-0.3186514559116014,	-0.535569025061867,	-0.6027124928297907,	-0.4570184534226387,	0.953705528021386,	-0.5691420434372769,	-0.28718516587580756,	-0.9632001067387674,	0.2335805391900283,	-0.5391892244378311,	0.4847034721316992,	0.168500416789773,	-0.15979289082048598,	0.41850566438972425,	0.9440910518067389,	0.6200782353145564,	-0.13868964067662737,	0.9154571996229015,	-0.9044837977902083,	0.46556360178337663,	0.35133289856278926,	0.05401033558831192,	0.1339749037447604,	0.445890498494282,	0.3652067003305257,	0.5906253601985791,	-0.7743089555964124,	-0.6016747356572583,	0.44263845794550716,	-0.21226535503433608,	-0.3216291937643605,	-0.6931086278567578,	-0.798851970777563,	-0.1855057862898446,	0.16583092893300755,	0.4814410419233621,	0.7407216835513775,	0.9330523208053558,	-0.0888702892955664,	-0.1991540018566338,	-0.4021207380731766,	0.6567201334862367,	0.8196823399815116,	0.5322733505025901,	-0.86337043137215,	0.2120530399516265,	0.8784322648567571,	-0.795714684509865,	0.9550790328799044,	-0.2866225512510203,	0.30837433667812064,	-0.7544285874396095,	0.9227043868271638,	0.3023628543432315,	0.8733556197886474,	0.817654610548818,
    	-0.12598437753173974,	0.8665219187748021,	0.41243449688471534,	-0.4657267385142949,	-0.6404671971680762,	0.31110670971769516,	0.644055632533985,	0.8158177341712622,	0.2523514431062559,	-0.6744549249012155,	-0.43670761373143496,	0.17641814589723892,	-0.7765011812575202,	-0.46095543509259973,	-0.9960311979084255,	0.793126748094007,	0.8606549164375583,	-0.31953209432521024,	-0.15483789828661543,	0.3403927262924109,	0.22247718304056163,	0.32184601225666576,	0.014125608009941226,	0.2699465098949414,	-0.7835174969795935,	0.9024014311513453,	-0.9331369713482778,	-0.8059820672082039,	0.6938935363417189,	0.22473750791601455,	0.40030290793377854,	0.10052025567236811,	0.6228028307496731,	0.2887641511978569,	-0.6050456288620749,	0.5466453239697793,	-0.3107915580566094,	0.7008204675109508,	-0.3174037097123865,	0.9435332700341295,	-0.5531408522308381,	0.2767998655895729,	-0.7252857172015899,	-0.7412018408044516,
    };

    // shape is (18,)
    [[maybe_unused]] float d_mha_att_b[]=
    {
    	0.30255146121218757,	0.4348328252113165,	-0.8010240572240188,	0.15505132828464085,	-0.1709343440354727,	-0.7296280388513319,	0.03522771750395237,	-0.8705504627968308,	0.7964160613190279,	-0.8111254245821189,	0.7257071617767945,	0.47694416094714853,	-0.770976947824699,	0.0853750818407768,	0.9403013445905344,	-0.5058455174138803,	0.1468515264773531,	0.1360702461191019,
    };

    // shape is (6, 6)
    [[maybe_unused]] float d_mha_proj_w[]=
    {
    	-0.3170149438694583,	-0.7645466800583802,	0.08900728280904135,	0.13572990918998928,	0.8290795294590052,	0.252801252383835,	-0.008752175622826597,	-0.6291109360156826,	-0.22244175217126294,	0.14540858053651418,	0.45458378043581615,	0.9754669326636882,	-0.6164227961281241,	0.3459680040385973,	-0.16933237088724318,	0.2730163508786929,	-0.9880592690960197,	0.4665912047641243,	0.6494009099542706,	-0.9083764825813139,	-0.2873148715330349,	-0.5591527172353177,	0.2976885388221209,	-0.8271853075126963,	-0.3218392012398139,	0.3171495527436883,	0.6608294740492997,	-0.6220826831541246,	-0.9707342621926138,	-0.5574577361242881,	0.5600866849820552,	-0.9918312804385114,	0.3798416430027025,	-0.5804151769200283,	0.6574830076704905,	0.615837907664331,
    };

    // shape is (6,)
    [[maybe_unused]] float d_mha_proj_b[]=
    {
    	0.8830014104853869,	-0.01057092902434209,	-0.03747542309176244,	0.636256264916347,	0.11960265778383694,	0.6645231879241347,
    };

    // shape is (6, 6)
    [[maybe_unused]] float d_mlp_c_fc_w[]=
    {
    	0.2223836966706143,	-0.25698438457207606,	0.5797648702138674,	0.16227944387439996,	0.39692839268240676,	-0.8805165820807277,	-0.6214698060325368,	-0.20362793315062766,	0.474105524222062,	0.104433803087568,	-0.5185875317274034,	0.8190184836657846,	-0.9495840500330022,	-0.4186602075104702,	-0.5987683828604038,	-0.2704861519635835,	0.7692708775402664,	0.37224808868226855,	-0.7205219876458311,	-0.624891396320554,	-0.9200352804234759,	-0.6179480890698945,	-0.6420313863635729,	-0.37586523921706627,	0.2526594689717483,	0.43468885575341054,	-0.7385230314783913,	0.08239817052911147,	0.6712680043436743,	-0.2724857228750692,	0.12088496497375312,	0.04517245064379338,	0.13674991110511958,	0.8057216766203104,	-0.44704404071715587,	0.0023609743915347092,
    };

    // shape is (6,)
    [[maybe_unused]] float d_mlp_c_fc_b[]=
    {
    	-0.9999911001857487,	-0.9157110832493733,	-0.36890831668212143,	0.18466675301531676,	-0.9448425682727348,	-0.22253169399471484,
    };

    // shape is (6, 6)
    [[maybe_unused]] float d_mlp_c_proj_w[]=
    {
    	0.542968010536466,	-0.9890696365105922,	0.6649463538785858,	0.48828953261383545,	0.464105817387535,	0.5649831362323523,	0.8485079843049239,	0.15838310460669325,	-0.6950327948039674,	0.42694946820711266,	-0.28141903421264014,	0.7999580234956498,	0.6812616164908885,	0.40954760943166435,	-0.7471483811477198,	-0.24000702320993095,	-0.20477690724078235,	0.7218709761579072,	0.8172395671233323,	-0.01538850028969474,	0.1104636079835346,	-0.9128849500149361,	-0.663222858977244,	0.2895631688739466,	-0.0677660626524983,	-0.8630975706125528,	-0.7930059265723357,	-0.7972899682071102,	-0.3480517472947635,	0.18700426508762558,	0.6112576797430189,	0.481368008865807,	-0.3344289134221101,	-0.8539711704067743,	-0.8600584953133581,	-0.6823744336757303,
    };

    // shape is (6,)
    [[maybe_unused]] float d_mlp_c_proj_b[]=
    {
    	0.16146607286176784,	-0.40462705069350635,	0.25111654945602524,	-0.848816686857877,	0.2969929302438452,	-0.4426582172685489,
    };

    // shape is (2, 6)
    [[maybe_unused]] float d_gt[]=
    {
    	1.5508114055530593,	-0.08975180003755023,	3.478599106146416,	-4.9277093156922085,	-1.3931217113315109,	-2.9717827677256174,	2.438293145968137,	-0.13932439870738444,	-0.10345661967588377,	-1.6761031188512812,	-1.3662770849725372,	-2.395764150469257,
    };

    [[maybe_unused]] float temp_gt[]=
    {
        1.325949,        -0.1109019,      3.26807, -3.536907,       -1.185472,       -1.645906,
        1.70879, -0.5092199,      0.03786933,      0.4697078,       -0.4368053,      -0.560122,
    };





    input.import_from( d_input );
    mlp_c_fc_w.import_from( d_mlp_c_fc_w );
    mlp_c_fc_b.import_from( d_mlp_c_fc_b );
    mlp_c_proj_w.import_from( d_mlp_c_proj_w );
    mlp_c_proj_b.import_from( d_mlp_c_proj_b );

    mha_att_w.import_from( d_mha_att_w );
    mha_att_b.import_from( d_mha_att_b );
    mha_proj_w.import_from( d_mha_proj_w );
    mha_proj_b.import_from( d_mha_proj_b );

    ln_1_alpha.import_from( d_ln_1_alpha );
    ln_1_beta.import_from( d_ln_1_beta );
    ln_2_alpha.import_from( d_ln_2_alpha );
    ln_2_beta.import_from( d_ln_2_beta );

    gt.import_from( temp_gt );
    //gt.import_from( d_gt );

    auto input_layer = Input( "InputLayer" );
    auto output_layer = Transformer( mlp_c_fc_w, mlp_c_fc_b, mlp_c_proj_w, mlp_c_proj_b,
                                     mha_att_w, mha_att_b, mha_proj_w, mha_proj_b,
                                     ln_1_alpha, ln_1_beta,
                                     ln_2_alpha, ln_2_beta,
                                     n_head,
                                     "Transformer_43"
                        )( input_layer );
    auto m = model( input_layer, output_layer );
    auto outputs = m.predict( input );

    //cuda_device_synchronize();

    auto output = outputs[0];
    output.save_txt( "./output.txt" );

    //debug
    //input.synchronize_to_host();
    //input.save_txt( "./debug_input.txt" );

    #if 1
    auto mat = view_2d{ reinterpret_cast<float*>( gt.data() ), static_cast<std::uint64_t>(n_seq), static_cast<std::uint64_t>(n_embd) };
    auto nat = view_2d{ reinterpret_cast<float*>( output.data() ), static_cast<std::uint64_t>(n_seq), static_cast<std::uint64_t>(n_embd) };
    for ( auto r : range( n_seq ) )
    {
        for ( auto c : range( n_embd ) )
        {
            if ( std::abs(mat[r][c]-nat[r][c]) > 0.1 )
            {
                spdlog::error( "gt[{}][{}]={}, pred[{}][{}]={}", r, c, mat[r][c], r, c, nat[r][c] );
            }
            REQUIRE( std::abs(mat[r][c]-nat[r][c]) < 0.1 );
        }
    }
    #endif
}

